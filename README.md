# CFDLLMBench

**CFDLLMBench** is a comprehensive benchmark suite designed to assess Large Language Models (LLMs) on key competencies relevant to computational fluid dynamics (CFD). It evaluates LLMs across three interconnected tasks:

- âœ… **CFDQuery**: Conceptual understanding through multiple-choice CFD questions.
- âœ… **CFDCodeBench**: Instruction-following ability to generate CFD code.
- âœ… **FoamBench**: End-to-end automation of OpenFOAM simulations from natural language.

---

## ğŸ” Motivation

While LLMs excel in general NLP, their ability to reason about scientific systems, generate structured physical code, and interface with domain-specific simulators like OpenFOAM remains underexplored.

**CFDLLMBench** bridges this gap by systematically measuring:
- Conceptual grasp of CFD principles (via QA),
- Structured code generation fidelity,
- Robustness of simulation execution and output.

---

## ğŸ§± Benchmark Components

### ğŸ“˜ CFDQuery

- 90 graduate-level multiple-choice questions from CFD lectures.
- Covers numerical methods, turbulence modeling, and solver theory.
- LLMs select the correct option from four choices.

### ğŸ”§ CFDCodeBench

- 24 natural language prompts describing CFD problems.
- Models must output executable CFD code (FDM, FVM, etc.).
- Evaluated on correctness, numerical stability, and output similarity.

### ğŸŒ€ FoamBench

- 110 Basic + 16 Advanced OpenFOAM simulation tasks.
- Benchmarks end-to-end tool use: case creation â†’ execution â†’ postprocessing.
- Assesses multi-step reasoning, domain adaptation, and recovery.

---

## ğŸ–¼ï¸ Benchmark Overview

![Benchmark Overview](figs/main_figure_v6.png)

---

## ğŸ“ˆ Main Results

![All Scores](figs/all_bars.png)

---

## ğŸ“„ License

This benchmark is open-source and released under license: BSD-3-Clause.

