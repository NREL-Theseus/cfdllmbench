# API Reference

Complete API documentation for CFDLLMBench components.

## Core Classes

### CFDQueryEvaluator

Evaluates LLM performance on conceptual CFD questions.

```python
class CFDQueryEvaluator:
    def __init__(self, question_file: str):
        """Initialize evaluator with question dataset."""
        
    def evaluate_model(self, model: LLMInterface) -> Dict[str, float]:
        """Evaluate model and return accuracy metrics."""
        
    def generate_report(self, results: Dict) -> str:
        """Generate detailed evaluation report."""
```

#### Methods

**`evaluate_model(model)`**
- **Parameters**: `model` (LLMInterface) - The language model to evaluate
- **Returns**: Dictionary with accuracy scores and detailed metrics
- **Example**:
```python
evaluator = CFDQueryEvaluator("data/cfdquery.json")
results = evaluator.evaluate_model(my_model)
print(f"Accuracy: {results['accuracy']:.2%}")
```

### CFDCodeBenchEvaluator

Assesses code generation capabilities for CFD problems.

```python
class CFDCodeBenchEvaluator:
    def __init__(self, problems_dir: str):
        """Initialize with CFD problem definitions."""
        
    def evaluate_code(self, generated_code: str, problem_id: str) -> Dict:
        """Evaluate generated code for specific problem."""
        
    def run_full_evaluation(self, model: LLMInterface) -> Dict:
        """Run complete evaluation suite."""
```

#### Methods

**`evaluate_code(generated_code, problem_id)`**
- **Parameters**: 
  - `generated_code` (str) - The code generated by LLM
  - `problem_id` (str) - Identifier for the CFD problem
- **Returns**: Evaluation metrics including execution success and accuracy
- **Example**:
```python
evaluator = CFDCodeBenchEvaluator("data/problems/")
result = evaluator.evaluate_code(llm_code, "heat_conduction_1d")
```

### FoamBenchEvaluator

Manages OpenFOAM simulation automation tasks.

```python
class FoamBenchEvaluator:
    def __init__(self, tasks_dir: str, openfoam_path: str):
        """Initialize with task definitions and OpenFOAM installation."""
        
    def run_task(self, task_id: str, model: LLMInterface) -> TaskResult:
        """Execute single FoamBench task."""
        
    def cleanup_case(self, case_dir: str) -> None:
        """Clean up OpenFOAM case directory."""
```

## Utility Functions

### Model Interface

```python
def load_model(model_name: str, **kwargs) -> LLMInterface:
    """Load and configure language model."""
    
def batch_evaluate(models: List[str], evaluators: List[Evaluator]) -> DataFrame:
    """Run batch evaluation across multiple models."""
```

### Data Processing

```python
def load_cfd_questions(file_path: str) -> List[Question]:
    """Load CFDQuery dataset."""
    
def parse_openfoam_case(case_dir: str) -> CaseConfig:
    """Parse OpenFOAM case configuration."""
    
def compute_similarity_metrics(generated: Array, reference: Array) -> Dict:
    """Compute numerical similarity between outputs."""
```

## Configuration

### Model Configuration

```python
# config.yaml
models:
  openai:
    api_key: "${OPENAI_API_KEY}"
    model: "gpt-4"
    temperature: 0.1
    max_tokens: 2048
    
  anthropic:
    api_key: "${ANTHROPIC_API_KEY}"  
    model: "claude-2"
    temperature: 0.1
```

### Evaluation Settings

```python
evaluation:
  timeout: 300  # seconds
  max_retries: 3
  parallel_jobs: 4
  
openfoam:
  version: "8"
  mpi_processes: 4
  case_timeout: 1800
```

## Error Handling

### Common Exceptions

```python
class CFDBenchError(Exception):
    """Base exception for CFDLLMBench."""
    
class ModelTimeoutError(CFDBenchError):
    """Raised when model response times out."""
    
class OpenFOAMError(CFDBenchError):
    """Raised when OpenFOAM execution fails."""
    
class CodeExecutionError(CFDBenchError):
    """Raised when generated code fails to execute."""
```

## Examples

### Complete Evaluation

```python
from cfdllmbench import CFDQueryEvaluator, CFDCodeBenchEvaluator, FoamBenchEvaluator
from cfdllmbench.models import OpenAIModel

# Initialize model
model = OpenAIModel("gpt-4", api_key="your-key")

# Run all evaluations
cfdquery_eval = CFDQueryEvaluator("data/cfdquery.json")
codebench_eval = CFDCodeBenchEvaluator("data/problems/")
foambench_eval = FoamBenchEvaluator("data/foam_tasks/", "/opt/openfoam8")

# Get results
query_results = cfdquery_eval.evaluate_model(model)
code_results = codebench_eval.run_full_evaluation(model)
foam_results = foambench_eval.run_all_tasks(model)

print(f"CFDQuery Accuracy: {query_results['accuracy']:.2%}")
print(f"CodeBench Success: {code_results['success_rate']:.2%}")
print(f"FoamBench Success: {foam_results['success_rate']:.2%}")
```
